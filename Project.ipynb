{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915994bc-ca4b-4bda-bcb8-c9f0457cf2a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Εξαμηνιαία Εργασία\n",
    "\n",
    "### Νικόλαος Καρακώστας 03120138\n",
    "### Μιχαήλ Δημητρόπουλος 03120119"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4512b7e-7e59-4fa8-b5b5-476b32a58c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ερώτημα 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b3a0f-a2f8-4faa-bc66-a4c4ed3e7a84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RDD APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31625e9c-5520-4261-ac5c-49f82fa72c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 25 and 64: 121093\n",
      "Between 18 and 24: 33605\n",
      "Younger than 18: 15928\n",
      "Older than 64: 5985\n",
      "Execution time: 8.609183073043823 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize SparkSession and SparkContext\n",
    "# Configure SparkSession with 4 executors\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1 RDD\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "header_10s = crime_data_10s.first()\n",
    "crime_data_10s = crime_data_10s.filter(lambda x: x != header_10s)\n",
    "crime_data_10s = crime_data_10s.map(lambda line: list(csv.reader([line]))[0])\n",
    "filtered_10s = crime_data_10s.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "useful_10s = filtered_10s.map(lambda x: [x[11], [x[0]]])\n",
    "\n",
    "# Load and process the data for 2020s\n",
    "crime_data_20s = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "header_20s = crime_data_20s.first()\n",
    "crime_data_20s = crime_data_20s.filter(lambda x: x != header_20s)\n",
    "crime_data_20s = crime_data_20s.map(lambda line: list(csv.reader([line]))[0])\n",
    "filtered_20s = crime_data_20s.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "useful_20s = filtered_20s.map(lambda x: [x[11], [x[0]]])\n",
    "\n",
    "# Combine the two RDDs\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# Function to categorize age into groups\n",
    "def categorize_age(age):\n",
    "    try:\n",
    "        age = int(age)  # Convert age to integer\n",
    "        if age < 18:\n",
    "            return \"Younger than 18\"\n",
    "        elif 18 <= age <= 24:\n",
    "            return \"Between 18 and 24\"\n",
    "        elif 25 <= age <= 64:\n",
    "            return \"Between 25 and 64\"\n",
    "        else:\n",
    "            return \"Older than 64\"\n",
    "    except ValueError:\n",
    "        return \"Unknown\"  # Handle cases where age is not a valid integer\n",
    "\n",
    "# Group and count by age categories\n",
    "grouped_data = combined_useful.map(lambda x: (categorize_age(x[0]), 1)) \\\n",
    "                              .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort the groups by count in descending order\n",
    "sorted_groups = grouped_data.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect and print the results\n",
    "results = sorted_groups.collect()\n",
    "for group, count in results:\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6c1b9-266e-4fc2-a270-a3e76ff79588",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b1d024-71f9-40e2-abc8-627f3aea1bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 25 and 64: 121093\n",
      "Between 18 and 24: 33605\n",
      "Younger than 18: 15928\n",
      "Older than 64: 5985\n",
      "Execution time: 14.52008867263794 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1 DataFrame\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_10s = crime_data_10s.filter(\n",
    "    col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\")\n",
    ").select(\n",
    "    col(\"Vict Age\").alias(\"age\"),\n",
    "    col(\"DR_NO\").alias(\"id\")\n",
    ")\n",
    "\n",
    "# Load and process the data for 2020s\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_20s = crime_data_20s.filter(\n",
    "    col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\")\n",
    ").select(\n",
    "    col(\"Vict Age\").alias(\"age\"),\n",
    "    col(\"DR_NO\").alias(\"id\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# Categorize age into groups\n",
    "categorized = combined_useful.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Younger than 18\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"Between 18 and 24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 64), \"Between 25 and 64\")\n",
    "    .when(col(\"age\") > 64, \"Older than 64\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Group by age group and count\n",
    "grouped_data = categorized.groupBy(\"age_group\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Sort the groups by count in descending order\n",
    "sorted_groups = grouped_data.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Collect and print results\n",
    "results = sorted_groups.collect()\n",
    "for row in results:\n",
    "    print(f\"{row['age_group']}: {row['count']}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf3b6f-9a1a-4e79-b8f4-a3c8c15ed449",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ερώτημα 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5b6fe-0da6-4630-b0c8-4a4f77657f63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059dd934-9572-48d9-85c4-39ed0c85dd35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|  AREA NAME|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.735499940695053|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 18.020742177963257 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, count, when, desc, rank\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 DataFrame\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data for the years 2010-2019\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load data for the years 2020-present\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = crime_data_10s.union(crime_data_20s)\n",
    "\n",
    "# Add a \"year\" column based on the \"DATE OCC\" column\n",
    "combined_data = combined_data.withColumn(\"year\", col(\"DATE OCC\").substr(7, 4).cast(\"int\"))\n",
    "\n",
    "# Filter out rows where \"year\" is null or invalid\n",
    "combined_data = combined_data.filter(col(\"year\").isNotNull())\n",
    "\n",
    "# Calculate total cases per year and precinct\n",
    "total_cases = combined_data.groupBy(\"year\", \"AREA NAME\").agg(\n",
    "    count(\"*\").alias(\"total_cases\")\n",
    ")\n",
    "\n",
    "# Filter completed cases (closed cases have a \"STATUS\" not equal to \"IC\")\n",
    "completed_cases = combined_data.filter(col(\"Status Desc\") != \"UNK\")\n",
    "completed_cases = combined_data.filter(col(\"Status Desc\") != \"Invest Cont\")\n",
    "\n",
    "# Calculate closed cases per year and precinct\n",
    "closed_cases = completed_cases.groupBy(\"year\", \"AREA NAME\").agg(\n",
    "    count(\"*\").alias(\"closed_cases\")\n",
    ")\n",
    "\n",
    "# Join total cases and closed cases\n",
    "case_rates = total_cases.join(\n",
    "    closed_cases,\n",
    "    on=[\"year\", \"AREA NAME\"],\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100\n",
    ")\n",
    "\n",
    "# Rank precincts by closed case rate per year\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"closed_case_rate\"))\n",
    "ranked_data = case_rates.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Select top 3 precincts per year\n",
    "top_3_precincts = ranked_data.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Sort results by year and rank\n",
    "result = top_3_precincts.orderBy(\"year\", \"rank\")\n",
    "\n",
    "# Show final result\n",
    "result.select(\"year\", \"AREA NAME\", \"closed_case_rate\", \"rank\").show(60)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419b714-c0a4-4b99-85f7-e2615e4df0e0",
   "metadata": {},
   "source": [
    "### SQL APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b8c8718-60bf-4841-b16c-3f3c922c88e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|   precinct|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 36.99755048751831 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data for the years 2010-2019\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load data for the years 2020-present\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data1 = crime_data_10s.union(crime_data_20s)\n",
    "\n",
    "# Add a \"year\" column based on the \"DATE OCC\" column\n",
    "combined_data = combined_data1.withColumn(\"year\", combined_data1[\"DATE OCC\"].substr(7, 4).cast(\"int\"))\n",
    "\n",
    "# Filter out rows where \"year\" is null or invalid\n",
    "combined_data = combined_data.filter(combined_data[\"year\"].isNotNull())\n",
    "\n",
    "# Register combined_data as a temporary view\n",
    "combined_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write SQL query for total cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW total_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS total_cases\n",
    "    FROM crime_data\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# Write SQL query for closed cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW closed_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS closed_cases\n",
    "    FROM crime_data\n",
    "    WHERE `Status Desc` != 'Invest Cont' AND `Status Desc` != 'UNK'\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Join total_cases and closed_cases to calculate closed_case_rate\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW case_rates AS\n",
    "    SELECT\n",
    "        t.year,\n",
    "        t.precinct,\n",
    "        t.total_cases,\n",
    "        c.closed_cases,\n",
    "        (c.closed_cases / t.total_cases) * 100 AS closed_case_rate\n",
    "    FROM total_cases t\n",
    "    LEFT JOIN closed_cases c\n",
    "    ON t.year = c.year AND t.precinct = c.precinct\n",
    "\"\"\")\n",
    "\n",
    "# Rank precincts by closed case rate per year\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW ranked_data AS\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM case_rates\n",
    "\"\"\")\n",
    "\n",
    "# Select top 3 precincts per year\n",
    "top_3_precincts = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        rank\n",
    "    FROM ranked_data\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY year, rank\n",
    "\"\"\")\n",
    "\n",
    "# Show final result\n",
    "top_3_precincts.show(60)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b851ad-1473-48a1-befd-4877c4b944a7",
   "metadata": {},
   "source": [
    "### Parquet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5a3e0f6-0297-46df-9872-f439c5ffa23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "|    DR_NO|           Date Rptd|            DATE OCC|TIME OCC|AREA |AREA NAME|Rpt Dist No|Part 1-2|Crm Cd|         Crm Cd Desc|       Mocodes|Vict Age|Vict Sex|Vict Descent|Premis Cd|         Premis Desc|Weapon Used Cd|         Weapon Desc|Status| Status Desc|Crm Cd 1|Crm Cd 2|Crm Cd 3|Crm Cd 4|            LOCATION|        Cross Street|    LAT|      LON|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "|  1307355|02/20/2010 12:00:...|02/20/2010 12:00:...|    1350|   13|   Newton|       1385|       2|   900|VIOLATION OF COUR...|0913 1814 2000|      48|       M|           H|      501|SINGLE FAMILY DWE...|          NULL|                NULL|    AA|Adult Arrest|     900|    NULL|    NULL|    NULL|300 E  GAGE      ...|                NULL|33.9825|-118.2695|\n",
      "| 11401303|09/13/2010 12:00:...|09/12/2010 12:00:...|      45|   14|  Pacific|       1485|       2|   740|VANDALISM - FELON...|          0329|       0|       M|           W|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     740|    NULL|    NULL|    NULL|SEPULVEDA        ...|MANCHESTER       ...|33.9599|-118.3962|\n",
      "| 70309629|08/09/2010 12:00:...|08/09/2010 12:00:...|    1515|   13|   Newton|       1324|       2|   946|OTHER MISCELLANEO...|          0344|       0|       M|           H|      103|               ALLEY|          NULL|                NULL|    IC| Invest Cont|     946|    NULL|    NULL|    NULL|1300 E  21ST     ...|                NULL|34.0224|-118.2524|\n",
      "| 90631215|01/05/2010 12:00:...|01/05/2010 12:00:...|     150|    6|Hollywood|        646|       2|   900|VIOLATION OF COUR...|1100 0400 1402|      47|       F|           W|      101|              STREET|           102|            HAND GUN|    IC| Invest Cont|     900|     998|    NULL|    NULL|CAHUENGA         ...|HOLLYWOOD        ...|34.1016|-118.3295|\n",
      "|100100501|01/03/2010 12:00:...|01/02/2010 12:00:...|    2100|    1|  Central|        176|       1|   122|     RAPE, ATTEMPTED|          0400|      47|       F|           H|      103|               ALLEY|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     122|    NULL|    NULL|    NULL|8TH              ...|SAN PEDRO        ...|34.0387|-118.2488|\n",
      "|100100506|01/05/2010 12:00:...|01/04/2010 12:00:...|    1650|    1|  Central|        162|       1|   442|SHOPLIFTING - PET...|     0344 1402|      23|       M|           B|      404|    DEPARTMENT STORE|          NULL|                NULL|    AA|Adult Arrest|     442|    NULL|    NULL|    NULL|700 W  7TH       ...|                NULL| 34.048|-118.2577|\n",
      "|100100508|01/08/2010 12:00:...|01/07/2010 12:00:...|    2005|    1|  Central|        182|       1|   330|BURGLARY FROM VEH...|          0344|      46|       M|           H|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     330|    NULL|    NULL|    NULL|PICO             ...|GRAND            ...|34.0389|-118.2643|\n",
      "|100100509|01/09/2010 12:00:...|01/08/2010 12:00:...|    2100|    1|  Central|        157|       1|   230|ASSAULT WITH DEAD...|          0416|      51|       M|           B|      710|       OTHER PREMISE|           500|UNKNOWN WEAPON/OT...|    AA|Adult Arrest|     230|    NULL|    NULL|    NULL|500    CROCKER   ...|                NULL|34.0435|-118.2427|\n",
      "|100100510|01/09/2010 12:00:...|01/09/2010 12:00:...|     230|    1|  Central|        171|       1|   230|ASSAULT WITH DEAD...|     0400 0416|      30|       M|           H|      108|         PARKING LOT|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     230|    NULL|    NULL|    NULL|800 W  OLYMPIC   ...|                NULL| 34.045| -118.264|\n",
      "|100100511|01/09/2010 12:00:...|01/06/2010 12:00:...|    2100|    1|  Central|        132|       1|   341|THEFT-GRAND ($950...|     0344 1402|      55|       M|           W|      710|       OTHER PREMISE|          NULL|                NULL|    IC| Invest Cont|     341|     998|    NULL|    NULL|200 S  OLIVE     ...|                NULL|34.0538|-118.2488|\n",
      "|100100521|01/14/2010 12:00:...|01/14/2010 12:00:...|    1445|    1|  Central|        118|       2|   624|BATTERY - SIMPLE ...|0400 0429 2000|      38|       F|           B|      101|              STREET|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     624|    NULL|    NULL|    NULL|     900 N  BROADWAY|                NULL| 34.064|-118.2375|\n",
      "|100100522|01/15/2010 12:00:...|01/14/2010 12:00:...|    2000|    1|  Central|        158|       1|   210|             ROBBERY|          0344|      40|       M|           H|      101|              STREET|           400|STRONG-ARM (HANDS...|    AO| Adult Other|     210|    NULL|    NULL|    NULL|ALAMEDA          ...|7TH              ...| 34.035|-118.2386|\n",
      "|100100523|01/15/2010 12:00:...|01/15/2010 12:00:...|     245|    1|  Central|        182|       2|   740|VANDALISM - FELON...|          0329|      24|       F|           W|      102|            SIDEWALK|          NULL|                NULL|    AA|Adult Arrest|     740|    NULL|    NULL|    NULL|1100 S  OLIVE    ...|                NULL|34.0409|-118.2609|\n",
      "|100100529|01/16/2010 12:00:...|01/15/2010 12:00:...|    1745|    1|  Central|        152|       2|   755|          BOMB SCARE|          0404|      29|       F|           B|      738|             LIBRARY|           500|UNKNOWN WEAPON/OT...|    IC| Invest Cont|     755|    NULL|    NULL|    NULL|600 W  5TH       ...|                NULL|34.0502| -118.254|\n",
      "|100100531|01/16/2010 12:00:...|01/15/2010 12:00:...|    2030|    1|  Central|        127|       1|   210|             ROBBERY|0344 0416 1218|      47|       M|           A|      102|            SIDEWALK|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     210|    NULL|    NULL|    NULL|                 1ST|         LOS ANGELES|34.0515|-118.2424|\n",
      "|100100535|01/17/2010 12:00:...|01/16/2010 12:00:...|    1735|    1|  Central|        185|       2|   946|OTHER MISCELLANEO...|          NULL|      41|       M|           W|      103|               ALLEY|          NULL|                NULL|    IC| Invest Cont|     946|     999|    NULL|    NULL|300 E  OLYMPIC   ...|                NULL|34.0389| -118.255|\n",
      "|100100552|01/23/2010 12:00:...|01/23/2010 12:00:...|    1225|    1|  Central|        192|       2|   237|CHILD NEGLECT (SE...|          1251|      11|       M|           H|      502|MULTI-UNIT DWELLI...|          NULL|                NULL|    IC| Invest Cont|     237|    NULL|    NULL|    NULL|1300 S  FLOWER   ...|                NULL|34.0401|-118.2668|\n",
      "|100100553|01/23/2010 12:00:...|01/23/2010 12:00:...|    1100|    1|  Central|        166|       1|   210|             ROBBERY|     0344 0416|      50|       M|           W|      101|              STREET|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     210|    NULL|    NULL|    NULL|600    SAN JULIAN...|                NULL|34.0428|-118.2461|\n",
      "|100100555|01/23/2010 12:00:...|01/23/2010 12:00:...|    2000|    1|  Central|        132|       1|   236|INTIMATE PARTNER ...|          2000|      18|       F|           W|      502|MULTI-UNIT DWELLI...|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     236|    NULL|    NULL|    NULL|200 S  GRAND     ...|                NULL|34.0545|-118.2499|\n",
      "|100100561|01/26/2010 12:00:...|01/26/2010 12:00:...|    1820|    1|  Central|        119|       1|   210|             ROBBERY|     0346 0400|      37|       M|           H|      102|            SIDEWALK|           400|STRONG-ARM (HANDS...|    AA|Adult Arrest|     210|    NULL|    NULL|    NULL|800 N  ALAMEDA   ...|                NULL|34.0563|-118.2374|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "group_number = \"30\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/closed_cases/\"\n",
    "combined_data1.coalesce(1).write.mode(\"overwrite\").parquet(s3_path)\n",
    "combined_data1_again = spark.read.parquet(s3_path)\n",
    "combined_data1_again.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976ffa3-1a03-408e-84b3-6ecdea2a7ef6",
   "metadata": {},
   "source": [
    "### SQL API with parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7093417-0eea-468c-9561-2cc42ad77fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|   precinct|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 11.3284010887146 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the Parquet file\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group30/closed_cases/part-00000-d33d87a5-babd-45f6-8b8a-fa82a74fc51f-c000.snappy.parquet\"\n",
    "combined_data = spark.read.parquet(s3_path)\n",
    "\n",
    "# Add a \"year\" column if not already present\n",
    "if \"year\" not in combined_data.columns:\n",
    "    combined_data = combined_data.withColumn(\"year\", combined_data[\"DATE OCC\"].substr(7, 4).cast(\"int\"))\n",
    "\n",
    "# Filter out rows where \"year\" is null or invalid\n",
    "combined_data = combined_data.filter(combined_data[\"year\"].isNotNull())\n",
    "\n",
    "# Register combined_data as a temporary view\n",
    "combined_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write SQL query for total cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW total_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS total_cases\n",
    "    FROM crime_data\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# Write SQL query for closed cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW closed_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS closed_cases\n",
    "    FROM crime_data\n",
    "    WHERE STATUS != 'IC'\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# Join total_cases and closed_cases to calculate closed_case_rate\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW case_rates AS\n",
    "    SELECT\n",
    "        t.year,\n",
    "        t.precinct,\n",
    "        t.total_cases,\n",
    "        c.closed_cases,\n",
    "        (c.closed_cases / t.total_cases) * 100 AS closed_case_rate\n",
    "    FROM total_cases t\n",
    "    LEFT JOIN closed_cases c\n",
    "    ON t.year = c.year AND t.precinct = c.precinct\n",
    "\"\"\")\n",
    "\n",
    "# Rank precincts by closed case rate per year\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW ranked_data AS\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM case_rates\n",
    "\"\"\")\n",
    "\n",
    "# Select top 3 precincts per year\n",
    "top_3_precincts = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        rank\n",
    "    FROM ranked_data\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY year, rank\n",
    "\"\"\")\n",
    "\n",
    "# Show final result\n",
    "top_3_precincts.show(60)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794c6c9-da3b-4060-81fb-a7ef6fe4d266",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ερώτημα 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca4528-effd-4ead-826d-dd906b8a4729",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f410f0d3-b7b2-4078-9f11-d469c784159d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------+-------------+---------------+-----------------+------------+--------------------+-----------------+\n",
      "|       Community|ZCTA10|POP_2010|Median_Income|Total_HOUSING10|Income_Per_Person|Total_Crimes|Community_Population|Crimes_Per_Person|\n",
      "+----------------+------+--------+-------------+---------------+-----------------+------------+--------------------+-----------------+\n",
      "|  Woodland Hills| 91364|   24978|        92830|          10593|         39368.57|       44974|               62169|         0.723415|\n",
      "|  Woodland Hills| 91367|   37191|        80083|          16996|         36597.31|       44974|               62169|         0.723415|\n",
      "|        Winnetka| 91306|   44957|        61529|          14351|         19641.05|       31974|               44957|         0.711213|\n",
      "| Wilshire Center| 90020|   10962|        38849|           5271|         18680.27|       40843|               45630|         0.895091|\n",
      "| Wilshire Center| 90010|     763|        45786|            475|         28503.74|       40843|               45630|         0.895091|\n",
      "| Wilshire Center| 90005|    9487|        31142|           4081|         13396.28|       40843|               45630|         0.895091|\n",
      "| Wilshire Center| 90004|   24418|        40612|           9458|         15730.54|       40843|               45630|         0.895091|\n",
      "|      Wilmington| 90744|   52517|        41569|          14091|         11153.51|       39582|               52517|         0.753699|\n",
      "|     Willowbrook| 90059|   12389|        32506|           3468|          9099.27|         462|               28712|         0.016091|\n",
      "|     Willowbrook| 90222|   16323|        37881|           3874|          8990.44|         462|               28712|         0.016091|\n",
      "|        Whittier| 90601|   25011|        71366|           9525|         27178.49|           2|               88436|           2.3E-5|\n",
      "|        Whittier| 90605|   11324|        65952|           3807|         22172.31|           2|               88436|           2.3E-5|\n",
      "|        Whittier| 90604|    1381|        65295|            390|         18439.57|           2|               88436|           2.3E-5|\n",
      "|        Whittier| 90606|    4989|        64393|           1612|         20806.08|           2|               88436|           2.3E-5|\n",
      "|        Whittier| 90602|   25668|        48109|           8400|         15743.95|           2|               88436|           2.3E-5|\n",
      "|        Whittier| 90603|   20063|        88862|           7007|         31035.04|           2|               88436|           2.3E-5|\n",
      "|        Westwood| 90024|   47450|        57202|          19932|         24028.46|       20698|               47450|         0.436207|\n",
      "|Westlake Village| 91361|    7055|       104807|           2970|         44121.44|           1|                8270|          1.21E-4|\n",
      "|Westlake Village| 91362|    1215|       103958|            414|         35422.73|           1|                8270|          1.21E-4|\n",
      "|        Westlake| 90057|   32246|        27890|          12301|          10639.3|       54794|               32246|          1.69925|\n",
      "+----------------+------+--------+-------------+---------------+-----------------+------------+--------------------+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, substring, round, regexp_replace, collect_list\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON and Income Data Aggregation\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona Context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Load GeoJSON Data (2010 Census Blocks) from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "census_blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten GeoJSON properties\n",
    "flattened_census_blocks_df = census_blocks_df.select(\n",
    "    [col(f\"properties.{col_name}\").alias(col_name) for col_name in census_blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]\n",
    ").drop(\"properties\").drop(\"type\")\n",
    "\n",
    "# Select required columns from Census Blocks\n",
    "census_selected_columns_df = flattened_census_blocks_df.select(\"ZCTA10\", \"POP_2010\", \"COMM\", \"HOUSING10\", \"geometry\")\n",
    "\n",
    "# Filter rows with non-empty ZCTA10 and non-zero POP_2010\n",
    "census_filtered_df = census_selected_columns_df.filter(\n",
    "    (col(\"ZCTA10\").isNotNull()) & (col(\"ZCTA10\") != \"\") & (col(\"POP_2010\") > 0)\n",
    ")\n",
    "\n",
    "# Group Census Blocks by ZCTA10 and COMM, summing POP_2010 and HOUSING10, and merging geometries\n",
    "census_grouped_df = census_filtered_df.groupBy(\"ZCTA10\", \"COMM\").agg(\n",
    "    sum(\"POP_2010\").alias(\"Total_POP_2010\"),\n",
    "    sum(\"HOUSING10\").alias(\"Total_HOUSING10\"),\n",
    "    ST_Union(collect_list(\"geometry\")).alias(\"Merged_Geometry\")  # Merge geometries using ST_Union\n",
    ")\n",
    "\n",
    "# Sort the grouped Census Blocks data by ZCTA10 and COMM\n",
    "sorted_census_df = census_grouped_df.orderBy(\"ZCTA10\", \"COMM\")\n",
    "\n",
    "# Load Income Data (2015) from S3\n",
    "income_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "income_data_df = spark.read.csv(\n",
    "    income_data_path,\n",
    "    header=True,\n",
    "    inferSchema=False  # Treat all columns as strings for preprocessing\n",
    ")\n",
    "\n",
    "# Rename columns for consistency and avoid ambiguity\n",
    "income_data_df = income_data_df.withColumnRenamed(\"Zip Code\", \"ZCTA10\").withColumnRenamed(\"Community\", \"COMM_income\")\n",
    "\n",
    "# Remove the first character of the \"Estimated Median Income\"\n",
    "income_data_df = income_data_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    col(\"Estimated Median Income\").substr(2, 100) # Keep characters starting from position 2\n",
    ")\n",
    "income_data_df = income_data_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \",\", \"\")\n",
    ")\n",
    "\n",
    "# Filter and clean Income Data (ensure ZCTA10 and COMM_income are not null)\n",
    "income_cleaned_df = income_data_df.filter(\n",
    "    (col(\"ZCTA10\").isNotNull()) & (col(\"COMM_income\").isNotNull()) & (col(\"Estimated Median Income\").isNotNull())\n",
    ")\n",
    "\n",
    "# Perform join between Census Blocks (περιοχές του LA) and Income Data (Median Income) based on ZCTA10 (Zip Code)\n",
    "joined_data_df = sorted_census_df.join(income_cleaned_df, [\"ZCTA10\"], \"inner\")\n",
    "\n",
    "# Filter for substring matching: COMM contained within COMM_income\n",
    "matching_data_df = joined_data_df.filter(col(\"COMM_income\").contains(col(\"COMM\")))\n",
    "\n",
    "# Select required columns, including HOUSING10\n",
    "final_result_df = matching_data_df.select(\n",
    "    col(\"ZCTA10\"),\n",
    "    col(\"COMM\").alias(\"Community\"),\n",
    "    col(\"Total_POP_2010\").cast(\"int\").alias(\"POP_2010\"),\n",
    "    col(\"Estimated Median Income\").alias(\"Median_Income\"),\n",
    "    col(\"Total_HOUSING10\").cast(\"int\")\n",
    ")\n",
    "\n",
    "final_result_df = final_result_df.filter(\n",
    "    (col(\"Community\").isNotNull()) & (col(\"Community\") != \"\") & (col(\"Community\") != \" \") & (col(\"Community\") != \"  \")\n",
    ")\n",
    "\n",
    "# Add \"Income per Person\" column with at most 2 decimals\n",
    "final_with_income_df = final_result_df.withColumn(\n",
    "    \"Income_Per_Person\",\n",
    "    round((col(\"Median_Income\") * col(\"Total_HOUSING10\")) / col(\"POP_2010\"), 2)  # Round to 2 decimals\n",
    ")\n",
    "\n",
    "# Sort the final result by ZCTA10\n",
    "sorted_final_result_df = final_with_income_df.orderBy(\"ZCTA10\")\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_10s = crime_data_10s.select(\n",
    "    col(\"DR_NO\").alias(\"id\"),\n",
    "    col(\"LAT\").alias(\"Latitude\"),\n",
    "    col(\"LON\").alias(\"Longtitude\")\n",
    ")\n",
    "\n",
    "# Load and process the data for 2020s\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "useful_20s = crime_data_20s.select(\n",
    "    col(\"DR_NO\").alias(\"id\"),\n",
    "    col(\"LAT\").alias(\"Latitude\"),\n",
    "    col(\"LON\").alias(\"Longtitude\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# ------------------------ About Crimes Per Person\n",
    "\n",
    "# Convert geometry column in flattened_census_blocks_df to GeometryType\n",
    "census_with_geometry = flattened_census_blocks_df.withColumn(\n",
    "    \"geometry\", col(\"geometry\").cast(GeometryType())\n",
    ")\n",
    "\n",
    "# Combine and prepare crime data with point geometry\n",
    "crime_points = combined_useful.withColumn(\n",
    "    \"point\", ST_Point(col(\"Longtitude\"), col(\"Latitude\"))\n",
    ")\n",
    "\n",
    "# Perform spatial join: Find crimes inside polygons\n",
    "crimes_with_communities = crime_points.join(\n",
    "    census_with_geometry,\n",
    "    ST_Contains(col(\"geometry\"), col(\"point\")),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Count the number of crimes per community\n",
    "crime_counts = crimes_with_communities.groupBy(\"COMM\").agg(\n",
    "    count(\"id\").alias(\"Total_Crimes\")\n",
    ")\n",
    "\n",
    "# Join crime counts with the final result DataFrame\n",
    "final_with_crime_data = sorted_final_result_df.join(\n",
    "    crime_counts,\n",
    "    sorted_final_result_df[\"Community\"] == crime_counts[\"COMM\"],\n",
    "    \"left\"\n",
    ").drop(\"COMM\")  # Drop duplicate community column\n",
    "\n",
    "# Fill null values for communities with no crimes\n",
    "final_with_crime_data = final_with_crime_data.fillna({\"Total_Crimes\": 0})\n",
    "\n",
    "# Calculate total population per community\n",
    "community_population = sorted_final_result_df.groupBy(\"Community\").agg(\n",
    "    sum(\"POP_2010\").alias(\"Community_Population\")\n",
    ")\n",
    "\n",
    "# Join the community population data back to the final DataFrame\n",
    "final_with_population = final_with_crime_data.join(\n",
    "    community_population,\n",
    "    on=\"Community\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate Crimes Per Person\n",
    "final_with_population_and_crimes = final_with_population.withColumn(\n",
    "    \"Crimes_Per_Person\",\n",
    "    round(col(\"Total_Crimes\") / col(\"Community_Population\"), 6)  # Round to 6 decimals for precision\n",
    ")\n",
    "\n",
    "# Handle potential divisions by zero (optional, if some communities have zero population)\n",
    "final_with_population_and_crimes = final_with_population_and_crimes.fillna({\"Crimes_Per_Person\": 0})\n",
    "\n",
    "# Show the final result\n",
    "final_with_population_and_crimes.orderBy(desc(\"Community\")).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80392cac-3831-4e70-92a5-8fc1281101b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ερώτημα 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "617017e1-491f-451e-bf9e-4ac0a7420c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---+\n",
      "|Victim Descent                |#  |\n",
      "+------------------------------+---+\n",
      "|White                         |431|\n",
      "|Other                         |49 |\n",
      "|Hispanic/Latin/Mexican        |42 |\n",
      "|Black                         |27 |\n",
      "|Unknown                       |25 |\n",
      "|Other Asian                   |12 |\n",
      "|American Indian/Alaskan Native|1  |\n",
      "|Chinese                       |1  |\n",
      "+------------------------------+---+\n",
      "\n",
      "+----------------------+----+\n",
      "|Victim Descent        |#   |\n",
      "+----------------------+----+\n",
      "|Hispanic/Latin/Mexican|1502|\n",
      "|Unknown               |117 |\n",
      "|White                 |103 |\n",
      "|Black                 |81  |\n",
      "|Other                 |35  |\n",
      "|Other Asian           |23  |\n",
      "|Filipino              |1   |\n",
      "|Chinese               |1   |\n",
      "+----------------------+----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, col, when\n",
    "\n",
    "# Select required columns, including HOUSING10\n",
    "query4_1 = matching_data_df.select(\n",
    "    col(\"ZCTA10\"),\n",
    "    col(\"COMM\"),\n",
    "    col(\"Merged_geometry\")\n",
    ")\n",
    "\n",
    "query4_2 = final_with_population_and_crimes.select(\n",
    "    col(\"ZCTA10\").alias(\"ZCTA\"),\n",
    "    col(\"Community\"),\n",
    "    col(\"Income_Per_Person\")\n",
    ")\n",
    "\n",
    "# Perform the join operation on ZCTA10 and Community\n",
    "joined_data = query4_1.join(\n",
    "    query4_2,\n",
    "    (query4_1[\"ZCTA10\"] == query4_2[\"ZCTA\"]) & (query4_1[\"COMM\"] == query4_2[\"Community\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"ZCTA10\"),\n",
    "    col(\"Community\"),\n",
    "    col(\"Income_Per_Person\"),\n",
    "    col(\"Merged_geometry\")\n",
    ")\n",
    "\n",
    "\n",
    "# Get the 3 highest incomes per person\n",
    "highest_income = joined_data.orderBy(desc(\"Income_Per_Person\")).limit(3)\n",
    "\n",
    "# Get the 3 lowest incomes per person with Income_Per_Person > 0\n",
    "lowest_income = joined_data.filter(\"Income_Per_Person > 0\").orderBy(\"Income_Per_Person\").limit(3)\n",
    "\n",
    "\n",
    "# Display the result\n",
    "#highest_income.show()\n",
    "#lowest_income.show()\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Add a \"year\" column and filter for the year 2015 and non-null Vict Descent\n",
    "crime_with_year = (\n",
    "    crime_data_10s\n",
    "    .withColumn(\"year\", col(\"DATE OCC\").substr(7, 4).cast(\"int\"))\n",
    "    .filter((col(\"year\") == 2015) & (col(\"Vict Descent\").isNotNull()))\n",
    "    .select(\n",
    "        col(\"DR_NO\"),\n",
    "        col(\"Vict Descent\"),\n",
    "        col(\"LAT\"),\n",
    "        col(\"LON\"),\n",
    "        col(\"year\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create point geometries for crimes\n",
    "crime_2015_points = crime_with_year.withColumn(\n",
    "    \"point\", ST_Point(col(\"LON\"), col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# Perform the spatial join using ST_Contains\n",
    "crimes_in_high_income = crime_2015_points.join(\n",
    "    highest_income,\n",
    "    ST_Contains(highest_income[\"Merged_geometry\"], crime_2015_points[\"point\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    crime_2015_points[\"DR_NO\"],\n",
    "    crime_2015_points[\"Vict Descent\"],\n",
    "    crime_2015_points[\"LAT\"],\n",
    "    crime_2015_points[\"LON\"],\n",
    "    highest_income[\"Community\"],\n",
    "    highest_income[\"Income_Per_Person\"]\n",
    ")\n",
    "\n",
    "# Perform the spatial join using ST_Contains\n",
    "crimes_in_low_income = crime_2015_points.join(\n",
    "    lowest_income,\n",
    "    ST_Contains(lowest_income[\"Merged_geometry\"], crime_2015_points[\"point\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    crime_2015_points[\"DR_NO\"],\n",
    "    crime_2015_points[\"Vict Descent\"],\n",
    "    crime_2015_points[\"LAT\"],\n",
    "    crime_2015_points[\"LON\"],\n",
    "    lowest_income[\"Community\"],\n",
    "    lowest_income[\"Income_Per_Person\"]\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "#crimes_in_high_income.show()\n",
    "#crimes_in_high_income.count()\n",
    "\n",
    "\n",
    "# Define a mapping dictionary for Victim Descent codes\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "# Build the when condition for all mappings\n",
    "descent_column = when(col(\"Vict Descent\") == \"A\", \"Other Asian\")\n",
    "for code, description in descent_mapping.items():\n",
    "    descent_column = descent_column.when(col(\"Vict Descent\") == code, description)\n",
    "\n",
    "# Replace codes with descriptions in the DataFrame\n",
    "highest_vict_descent = crimes_in_high_income.groupBy(\"Vict Descent\").count().withColumn(\n",
    "    \"Victim Descent\", descent_column\n",
    ").select(\n",
    "    col(\"Victim Descent\"),\n",
    "    col(\"count\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "# Replace codes with descriptions in the DataFrame\n",
    "lowest_vict_descent = crimes_in_low_income.groupBy(\"Vict Descent\").count().withColumn(\n",
    "    \"Victim Descent\", descent_column\n",
    ").select(\n",
    "    col(\"Victim Descent\"),\n",
    "    col(\"count\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "# Display the result\n",
    "highest_vict_descent.show(truncate=False)\n",
    "lowest_vict_descent.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fdac5-fde7-4ea9-97c6-0cb4ad17fa1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ερώτημα 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77fd907-c5c0-4098-9470-8a4d24aeeba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------+------------------+\n",
      "|Closest Area Name|Number of Closest Crimes|   Median Distance|\n",
      "+-----------------+------------------------+------------------+\n",
      "|      77th Street|                  206784| 2.292726866601563|\n",
      "|        Southwest|                  192226|2.3723678063581852|\n",
      "|          Pacific|                  170903| 4.342278455155561|\n",
      "|          Central|                  166698| 1.093428009365451|\n",
      "|      N Hollywood|                  164532|2.6969736366261494|\n",
      "|        Southeast|                  161051|1.8476966788287785|\n",
      "|        Hollywood|                  150663| 1.439317682870132|\n",
      "|           Newton|                  148757|1.7463468819264603|\n",
      "|          Olympic|                  144962|1.9791002660074106|\n",
      "|          Mission|                  143600| 4.228196896970822|\n",
      "|        Northeast|                  142732| 3.972583911782645|\n",
      "|         Van Nuys|                  142194|2.5715909394184853|\n",
      "|          Topanga|                  138642|3.5032555344892358|\n",
      "|       Devonshire|                  137881|3.3232967291195066|\n",
      "|         Wilshire|                  136199|2.4558662959589768|\n",
      "|          Rampart|                  135948|1.3388016824808497|\n",
      "|          West LA|                  134259| 3.111039069941839|\n",
      "|           Harbor|                  132911| 3.716521204833013|\n",
      "|      West Valley|                  131502| 3.494125640654584|\n",
      "|       Hollenbeck|                  114517|2.4314867007517815|\n",
      "|         Foothill|                  112919|3.8182090552354824|\n",
      "+-----------------+------------------------+------------------+\n",
      "\n",
      "Execution time: 81.64516568183899 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, expr, first, desc\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "import time\n",
    "\n",
    "# Initialize Spark session and Sedona context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeDataAnalysis\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load police stations data\n",
    "police_stations_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "police_stations_df = spark.read.csv(\n",
    "    police_stations_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load and process the crime data for 2010s\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load and process the crime data for 2020s\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_10s = crime_data_10s.select(\n",
    "    col(\"DR_NO\").alias(\"Crime ID\"),\n",
    "    col(\"AREA \").alias(\"Area\"),\n",
    "    col(\"AREA NAME\").alias(\"Area Name\"),\n",
    "    col(\"LAT\").alias(\"Latitude\"),\n",
    "    col(\"LON\").alias(\"Longtitude\")\n",
    ")\n",
    "\n",
    "useful_20s = crime_data_20s.select(\n",
    "    col(\"DR_NO\").alias(\"Crime ID\"),\n",
    "    col(\"AREA\").alias(\"Area\"),\n",
    "    col(\"AREA NAME\").alias(\"Area Name\"),\n",
    "    col(\"LAT\").alias(\"Latitude\"),\n",
    "    col(\"LON\").alias(\"Longtitude\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# Filter out Null Island records\n",
    "filtered_crimes = combined_useful.filter(\n",
    "    (col(\"Latitude\") != 0) & (col(\"Longtitude\") != 0)\n",
    ")\n",
    "\n",
    "# Create geospatial points for crimes\n",
    "crime_points = filtered_crimes.withColumn(\n",
    "    \"crime_geom\", ST_Point(col(\"Longtitude\"), col(\"Latitude\"))\n",
    ")\n",
    "\n",
    "# Create geospatial points for police stations\n",
    "station_points = police_stations_df.withColumn(\n",
    "    \"station_geom\", ST_Point(col(\"X\"), col(\"Y\"))\n",
    ")\n",
    "\n",
    "# Perform a Cartesian join to calculate distances between each crime and police station\n",
    "\n",
    "# Earth's radius in kilometers\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "# Perform a Cartesian join to calculate distances in kilometers\n",
    "crime_station_distances = crime_points.crossJoin(station_points).withColumn(\n",
    "    \"distance\", \n",
    "    ST_Distance(col(\"crime_geom\"), col(\"station_geom\")) * (3.141592653589793 / 180) * EARTH_RADIUS_KM\n",
    ")\n",
    "#crime_station_distances.show()\n",
    "\n",
    "# Find the minimum distance for each crime and keep Area and Area Name\n",
    "min_distances = crime_station_distances.groupBy(\"Crime ID\").agg(\n",
    "    expr(\"min(distance)\").alias(\"min_distance\"),\n",
    "    first(\"Area\").alias(\"Closest Area\"),\n",
    "    first(\"Area Name\").alias(\"Closest Area Name\")\n",
    ")\n",
    "\n",
    "min_distances = min_distances.orderBy(\"Closest Area\")\n",
    "# Show the result\n",
    "#min_distances.show()\n",
    "#min_distances.count()\n",
    "\n",
    "# Step 1: Count of tuples for each closest area name\n",
    "area_counts = min_distances.groupBy(\"Closest Area Name\").agg(\n",
    "    count(\"Crime ID\").alias(\"Number of Closest Crimes\")\n",
    ")\n",
    "#area_counts.show(50)\n",
    "\n",
    "# Step 2: Median distance for each closest area name\n",
    "area_median_distances = min_distances.groupBy(\"Closest Area Name\").agg(\n",
    "    expr(\"percentile_approx(min_distance, 0.5)\").alias(\"Median Distance\")\n",
    ")\n",
    "\n",
    "# Step 3: Join counts and medians, and order by Number of Closest Crimes in descending order\n",
    "final_table = area_counts.join(\n",
    "    area_median_distances,\n",
    "    on=\"Closest Area Name\"\n",
    ").orderBy(desc(\"Number of Closest Crimes\"))\n",
    "\n",
    "final_table.show(21)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906f673-39ba-433a-97cb-f120335f3e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
