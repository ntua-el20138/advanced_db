{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915994bc-ca4b-4bda-bcb8-c9f0457cf2a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Εξαμηνιαία Εργασία\n",
    "\n",
    "### Νικόλαος Καρακώστας 03120138\n",
    "### Μιχαήλ Δημητρόπουλος 03120119"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4512b7e-7e59-4fa8-b5b5-476b32a58c71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ερώτημα 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b3a0f-a2f8-4faa-bc66-a4c4ed3e7a84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RDD APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31625e9c-5520-4261-ac5c-49f82fa72c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 25 and 64: 121093\n",
      "Between 18 and 24: 33605\n",
      "Younger than 18: 15928\n",
      "Older than 64: 5985\n",
      "Execution time: 7.186655759811401 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize SparkSession and SparkContext\n",
    "# Configure SparkSession with 4 executors\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1 RDD\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "header_10s = crime_data_10s.first()\n",
    "crime_data_10s = crime_data_10s.filter(lambda x: x != header_10s)\n",
    "crime_data_10s = crime_data_10s.map(lambda line: list(csv.reader([line]))[0])\n",
    "filtered_10s = crime_data_10s.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "useful_10s = filtered_10s.map(lambda x: [x[11], [x[0]]])\n",
    "\n",
    "# Load and process the data for 2020s\n",
    "crime_data_20s = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "header_20s = crime_data_20s.first()\n",
    "crime_data_20s = crime_data_20s.filter(lambda x: x != header_20s)\n",
    "crime_data_20s = crime_data_20s.map(lambda line: list(csv.reader([line]))[0])\n",
    "filtered_20s = crime_data_20s.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "useful_20s = filtered_20s.map(lambda x: [x[11], [x[0]]])\n",
    "\n",
    "# Combine the two RDDs\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# Function to categorize age into groups\n",
    "def categorize_age(age):\n",
    "    try:\n",
    "        age = int(age)  # Convert age to integer\n",
    "        if age < 18:\n",
    "            return \"Younger than 18\"\n",
    "        elif 18 <= age <= 24:\n",
    "            return \"Between 18 and 24\"\n",
    "        elif 25 <= age <= 64:\n",
    "            return \"Between 25 and 64\"\n",
    "        else:\n",
    "            return \"Older than 64\"\n",
    "    except ValueError:\n",
    "        return \"Unknown\"  # Handle cases where age is not a valid integer\n",
    "\n",
    "# Group and count by age categories\n",
    "grouped_data = combined_useful.map(lambda x: (categorize_age(x[0]), 1)) \\\n",
    "                              .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort the groups by count in descending order\n",
    "sorted_groups = grouped_data.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect and print the results\n",
    "results = sorted_groups.collect()\n",
    "for group, count in results:\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6c1b9-266e-4fc2-a270-a3e76ff79588",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b1d024-71f9-40e2-abc8-627f3aea1bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 25 and 64: 121093\n",
      "Between 18 and 24: 33605\n",
      "Younger than 18: 15928\n",
      "Older than 64: 5985\n",
      "Execution time: 3.670485734939575 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1 DataFrame\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_10s = crime_data_10s.filter(\n",
    "    col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\")\n",
    ").select(\n",
    "    col(\"Vict Age\").alias(\"age\"),\n",
    "    col(\"DR_NO\").alias(\"id\")\n",
    ")\n",
    "\n",
    "# Load and process the data for 2020s\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_20s = crime_data_20s.filter(\n",
    "    col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\")\n",
    ").select(\n",
    "    col(\"Vict Age\").alias(\"age\"),\n",
    "    col(\"DR_NO\").alias(\"id\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# Categorize age into groups\n",
    "categorized = combined_useful.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 18, \"Younger than 18\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"Between 18 and 24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 64), \"Between 25 and 64\")\n",
    "    .when(col(\"age\") > 64, \"Older than 64\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Group by age group and count\n",
    "grouped_data = categorized.groupBy(\"age_group\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Sort the groups by count in descending order\n",
    "sorted_groups = grouped_data.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Collect and print results\n",
    "results = sorted_groups.collect()\n",
    "for row in results:\n",
    "    print(f\"{row['age_group']}: {row['count']}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf3b6f-9a1a-4e79-b8f4-a3c8c15ed449",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ερώτημα 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5b6fe-0da6-4630-b0c8-4a4f77657f63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "059dd934-9572-48d9-85c4-39ed0c85dd35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|  AREA NAME|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.735499940695053|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 4.502016544342041 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, count, when, desc, rank\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 DataFrame\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data for the years 2010-2019\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load data for the years 2020-present\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = crime_data_10s.union(crime_data_20s)\n",
    "\n",
    "# Add a \"year\" column based on the \"DATE OCC\" column\n",
    "combined_data = combined_data.withColumn(\"year\", col(\"DATE OCC\").substr(7, 4).cast(\"int\"))\n",
    "\n",
    "# Filter out rows where \"year\" is null or invalid\n",
    "combined_data = combined_data.filter(col(\"year\").isNotNull())\n",
    "\n",
    "# Calculate total cases per year and precinct\n",
    "total_cases = combined_data.groupBy(\"year\", \"AREA NAME\").agg(\n",
    "    count(\"*\").alias(\"total_cases\")\n",
    ")\n",
    "\n",
    "# Filter completed cases (closed cases have a \"STATUS\" not equal to \"IC\")\n",
    "completed_cases = combined_data.filter(col(\"Status Desc\") != \"UNK\")\n",
    "completed_cases = combined_data.filter(col(\"Status Desc\") != \"Invest Cont\")\n",
    "\n",
    "# Calculate closed cases per year and precinct\n",
    "closed_cases = completed_cases.groupBy(\"year\", \"AREA NAME\").agg(\n",
    "    count(\"*\").alias(\"closed_cases\")\n",
    ")\n",
    "\n",
    "# Join total cases and closed cases\n",
    "case_rates = total_cases.join(\n",
    "    closed_cases,\n",
    "    on=[\"year\", \"AREA NAME\"],\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100\n",
    ")\n",
    "\n",
    "# Rank precincts by closed case rate per year\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"closed_case_rate\"))\n",
    "ranked_data = case_rates.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Select top 3 precincts per year\n",
    "top_3_precincts = ranked_data.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Sort results by year and rank\n",
    "result = top_3_precincts.orderBy(\"year\", \"rank\")\n",
    "\n",
    "# Show final result\n",
    "result.select(\"year\", \"AREA NAME\", \"closed_case_rate\", \"rank\").show(60)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419b714-c0a4-4b99-85f7-e2615e4df0e0",
   "metadata": {},
   "source": [
    "### SQL APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b8c8718-60bf-4841-b16c-3f3c922c88e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|   precinct|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 4.155921220779419 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data for the years 2010-2019\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load data for the years 2020-present\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data1 = crime_data_10s.union(crime_data_20s)\n",
    "\n",
    "# Add a \"year\" column based on the \"DATE OCC\" column\n",
    "combined_data = combined_data1.withColumn(\"year\", combined_data1[\"DATE OCC\"].substr(7, 4).cast(\"int\"))\n",
    "\n",
    "# Filter out rows where \"year\" is null or invalid\n",
    "combined_data = combined_data.filter(combined_data[\"year\"].isNotNull())\n",
    "\n",
    "# Register combined_data as a temporary view\n",
    "combined_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write SQL query for total cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW total_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS total_cases\n",
    "    FROM crime_data\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# Write SQL query for closed cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW closed_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS closed_cases\n",
    "    FROM crime_data\n",
    "    WHERE `Status Desc` != 'Invest Cont' AND `Status Desc` != 'UNK'\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Join total_cases and closed_cases to calculate closed_case_rate\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW case_rates AS\n",
    "    SELECT\n",
    "        t.year,\n",
    "        t.precinct,\n",
    "        t.total_cases,\n",
    "        c.closed_cases,\n",
    "        (c.closed_cases / t.total_cases) * 100 AS closed_case_rate\n",
    "    FROM total_cases t\n",
    "    LEFT JOIN closed_cases c\n",
    "    ON t.year = c.year AND t.precinct = c.precinct\n",
    "\"\"\")\n",
    "\n",
    "# Rank precincts by closed case rate per year\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW ranked_data AS\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM case_rates\n",
    "\"\"\")\n",
    "\n",
    "# Select top 3 precincts per year\n",
    "top_3_precincts = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        rank\n",
    "    FROM ranked_data\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY year, rank\n",
    "\"\"\")\n",
    "\n",
    "# Show final result\n",
    "top_3_precincts.show(60)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b851ad-1473-48a1-befd-4877c4b944a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parquet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5a3e0f6-0297-46df-9872-f439c5ffa23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "|    DR_NO|           Date Rptd|            DATE OCC|TIME OCC|AREA |AREA NAME|Rpt Dist No|Part 1-2|Crm Cd|         Crm Cd Desc|       Mocodes|Vict Age|Vict Sex|Vict Descent|Premis Cd|         Premis Desc|Weapon Used Cd|         Weapon Desc|Status| Status Desc|Crm Cd 1|Crm Cd 2|Crm Cd 3|Crm Cd 4|            LOCATION|        Cross Street|    LAT|      LON|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "|  1307355|02/20/2010 12:00:...|02/20/2010 12:00:...|    1350|   13|   Newton|       1385|       2|   900|VIOLATION OF COUR...|0913 1814 2000|      48|       M|           H|      501|SINGLE FAMILY DWE...|          NULL|                NULL|    AA|Adult Arrest|     900|    NULL|    NULL|    NULL|300 E  GAGE      ...|                NULL|33.9825|-118.2695|\n",
      "| 11401303|09/13/2010 12:00:...|09/12/2010 12:00:...|      45|   14|  Pacific|       1485|       2|   740|VANDALISM - FELON...|          0329|       0|       M|           W|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     740|    NULL|    NULL|    NULL|SEPULVEDA        ...|MANCHESTER       ...|33.9599|-118.3962|\n",
      "| 70309629|08/09/2010 12:00:...|08/09/2010 12:00:...|    1515|   13|   Newton|       1324|       2|   946|OTHER MISCELLANEO...|          0344|       0|       M|           H|      103|               ALLEY|          NULL|                NULL|    IC| Invest Cont|     946|    NULL|    NULL|    NULL|1300 E  21ST     ...|                NULL|34.0224|-118.2524|\n",
      "| 90631215|01/05/2010 12:00:...|01/05/2010 12:00:...|     150|    6|Hollywood|        646|       2|   900|VIOLATION OF COUR...|1100 0400 1402|      47|       F|           W|      101|              STREET|           102|            HAND GUN|    IC| Invest Cont|     900|     998|    NULL|    NULL|CAHUENGA         ...|HOLLYWOOD        ...|34.1016|-118.3295|\n",
      "|100100501|01/03/2010 12:00:...|01/02/2010 12:00:...|    2100|    1|  Central|        176|       1|   122|     RAPE, ATTEMPTED|          0400|      47|       F|           H|      103|               ALLEY|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     122|    NULL|    NULL|    NULL|8TH              ...|SAN PEDRO        ...|34.0387|-118.2488|\n",
      "|100100506|01/05/2010 12:00:...|01/04/2010 12:00:...|    1650|    1|  Central|        162|       1|   442|SHOPLIFTING - PET...|     0344 1402|      23|       M|           B|      404|    DEPARTMENT STORE|          NULL|                NULL|    AA|Adult Arrest|     442|    NULL|    NULL|    NULL|700 W  7TH       ...|                NULL| 34.048|-118.2577|\n",
      "|100100508|01/08/2010 12:00:...|01/07/2010 12:00:...|    2005|    1|  Central|        182|       1|   330|BURGLARY FROM VEH...|          0344|      46|       M|           H|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     330|    NULL|    NULL|    NULL|PICO             ...|GRAND            ...|34.0389|-118.2643|\n",
      "|100100509|01/09/2010 12:00:...|01/08/2010 12:00:...|    2100|    1|  Central|        157|       1|   230|ASSAULT WITH DEAD...|          0416|      51|       M|           B|      710|       OTHER PREMISE|           500|UNKNOWN WEAPON/OT...|    AA|Adult Arrest|     230|    NULL|    NULL|    NULL|500    CROCKER   ...|                NULL|34.0435|-118.2427|\n",
      "|100100510|01/09/2010 12:00:...|01/09/2010 12:00:...|     230|    1|  Central|        171|       1|   230|ASSAULT WITH DEAD...|     0400 0416|      30|       M|           H|      108|         PARKING LOT|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     230|    NULL|    NULL|    NULL|800 W  OLYMPIC   ...|                NULL| 34.045| -118.264|\n",
      "|100100511|01/09/2010 12:00:...|01/06/2010 12:00:...|    2100|    1|  Central|        132|       1|   341|THEFT-GRAND ($950...|     0344 1402|      55|       M|           W|      710|       OTHER PREMISE|          NULL|                NULL|    IC| Invest Cont|     341|     998|    NULL|    NULL|200 S  OLIVE     ...|                NULL|34.0538|-118.2488|\n",
      "|100100521|01/14/2010 12:00:...|01/14/2010 12:00:...|    1445|    1|  Central|        118|       2|   624|BATTERY - SIMPLE ...|0400 0429 2000|      38|       F|           B|      101|              STREET|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     624|    NULL|    NULL|    NULL|     900 N  BROADWAY|                NULL| 34.064|-118.2375|\n",
      "|100100522|01/15/2010 12:00:...|01/14/2010 12:00:...|    2000|    1|  Central|        158|       1|   210|             ROBBERY|          0344|      40|       M|           H|      101|              STREET|           400|STRONG-ARM (HANDS...|    AO| Adult Other|     210|    NULL|    NULL|    NULL|ALAMEDA          ...|7TH              ...| 34.035|-118.2386|\n",
      "|100100523|01/15/2010 12:00:...|01/15/2010 12:00:...|     245|    1|  Central|        182|       2|   740|VANDALISM - FELON...|          0329|      24|       F|           W|      102|            SIDEWALK|          NULL|                NULL|    AA|Adult Arrest|     740|    NULL|    NULL|    NULL|1100 S  OLIVE    ...|                NULL|34.0409|-118.2609|\n",
      "|100100529|01/16/2010 12:00:...|01/15/2010 12:00:...|    1745|    1|  Central|        152|       2|   755|          BOMB SCARE|          0404|      29|       F|           B|      738|             LIBRARY|           500|UNKNOWN WEAPON/OT...|    IC| Invest Cont|     755|    NULL|    NULL|    NULL|600 W  5TH       ...|                NULL|34.0502| -118.254|\n",
      "|100100531|01/16/2010 12:00:...|01/15/2010 12:00:...|    2030|    1|  Central|        127|       1|   210|             ROBBERY|0344 0416 1218|      47|       M|           A|      102|            SIDEWALK|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     210|    NULL|    NULL|    NULL|                 1ST|         LOS ANGELES|34.0515|-118.2424|\n",
      "|100100535|01/17/2010 12:00:...|01/16/2010 12:00:...|    1735|    1|  Central|        185|       2|   946|OTHER MISCELLANEO...|          NULL|      41|       M|           W|      103|               ALLEY|          NULL|                NULL|    IC| Invest Cont|     946|     999|    NULL|    NULL|300 E  OLYMPIC   ...|                NULL|34.0389| -118.255|\n",
      "|100100552|01/23/2010 12:00:...|01/23/2010 12:00:...|    1225|    1|  Central|        192|       2|   237|CHILD NEGLECT (SE...|          1251|      11|       M|           H|      502|MULTI-UNIT DWELLI...|          NULL|                NULL|    IC| Invest Cont|     237|    NULL|    NULL|    NULL|1300 S  FLOWER   ...|                NULL|34.0401|-118.2668|\n",
      "|100100553|01/23/2010 12:00:...|01/23/2010 12:00:...|    1100|    1|  Central|        166|       1|   210|             ROBBERY|     0344 0416|      50|       M|           W|      101|              STREET|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     210|    NULL|    NULL|    NULL|600    SAN JULIAN...|                NULL|34.0428|-118.2461|\n",
      "|100100555|01/23/2010 12:00:...|01/23/2010 12:00:...|    2000|    1|  Central|        132|       1|   236|INTIMATE PARTNER ...|          2000|      18|       F|           W|      502|MULTI-UNIT DWELLI...|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     236|    NULL|    NULL|    NULL|200 S  GRAND     ...|                NULL|34.0545|-118.2499|\n",
      "|100100561|01/26/2010 12:00:...|01/26/2010 12:00:...|    1820|    1|  Central|        119|       1|   210|             ROBBERY|     0346 0400|      37|       M|           H|      102|            SIDEWALK|           400|STRONG-ARM (HANDS...|    AA|Adult Arrest|     210|    NULL|    NULL|    NULL|800 N  ALAMEDA   ...|                NULL|34.0563|-118.2374|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "group_number = \"30\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/closed_cases/\"\n",
    "combined_data1.coalesce(1).write.mode(\"overwrite\").parquet(s3_path)\n",
    "combined_data1_again = spark.read.parquet(s3_path)\n",
    "combined_data1_again.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976ffa3-1a03-408e-84b3-6ecdea2a7ef6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SQL API with parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7093417-0eea-468c-9561-2cc42ad77fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|   precinct|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 2.043264865875244 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the Parquet file\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group30/closed_cases/part-00000-b293af4c-cae0-4a69-b32c-1d18c84d36a0-c000.snappy.parquet\"\n",
    "combined_data = spark.read.parquet(s3_path)\n",
    "\n",
    "# Add a \"year\" column if not already present\n",
    "if \"year\" not in combined_data.columns:\n",
    "    combined_data = combined_data.withColumn(\"year\", combined_data[\"DATE OCC\"].substr(7, 4).cast(\"int\"))\n",
    "\n",
    "# Filter out rows where \"year\" is null or invalid\n",
    "combined_data = combined_data.filter(combined_data[\"year\"].isNotNull())\n",
    "\n",
    "# Register combined_data as a temporary view\n",
    "combined_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write SQL query for total cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW total_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS total_cases\n",
    "    FROM crime_data\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# Write SQL query for closed cases per year and precinct\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW closed_cases AS\n",
    "    SELECT\n",
    "        year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS closed_cases\n",
    "    FROM crime_data\n",
    "    WHERE STATUS != 'IC'\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# Join total_cases and closed_cases to calculate closed_case_rate\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW case_rates AS\n",
    "    SELECT\n",
    "        t.year,\n",
    "        t.precinct,\n",
    "        t.total_cases,\n",
    "        c.closed_cases,\n",
    "        (c.closed_cases / t.total_cases) * 100 AS closed_case_rate\n",
    "    FROM total_cases t\n",
    "    LEFT JOIN closed_cases c\n",
    "    ON t.year = c.year AND t.precinct = c.precinct\n",
    "\"\"\")\n",
    "\n",
    "# Rank precincts by closed case rate per year\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW ranked_data AS\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM case_rates\n",
    "\"\"\")\n",
    "\n",
    "# Select top 3 precincts per year\n",
    "top_3_precincts = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        rank\n",
    "    FROM ranked_data\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY year, rank\n",
    "\"\"\")\n",
    "\n",
    "# Show final result\n",
    "top_3_precincts.show(60)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794c6c9-da3b-4060-81fb-a7ef6fe4d266",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ερώτημα 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca4528-effd-4ead-826d-dd906b8a4729",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40ba5aed-adc5-4597-9a2d-304f4d02e185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+\n",
      "|           Community|Income_Per_Person|Crimes_Per_Person|\n",
      "+--------------------+-----------------+-----------------+\n",
      "|     Adams-Normandie|          8791.46|             0.74|\n",
      "|              Alsace|          11239.5|             0.55|\n",
      "|Angeles National ...|         28117.65|            11.35|\n",
      "|    Angelino Heights|         18411.55|             0.63|\n",
      "|              Arleta|         12110.78|             0.44|\n",
      "|     Atwater Village|          28477.2|             0.69|\n",
      "|       Baldwin Hills|          17302.7|             1.19|\n",
      "|             Bel Air|         63041.34|             0.43|\n",
      "|       Beverly Crest|         60947.49|             0.37|\n",
      "|         Beverlywood|         29267.82|             0.52|\n",
      "|       Boyle Heights|          8434.46|             0.72|\n",
      "|           Brentwood|         60840.62|              0.5|\n",
      "|           Brookside|         18138.62|             0.89|\n",
      "|    Cadillac-Corning|         19572.78|             0.67|\n",
      "|         Canoga Park|         19656.96|              0.9|\n",
      "|             Carthay|         49841.16|             0.93|\n",
      "|             Central|          6972.52|             0.71|\n",
      "|        Century City|         46103.51|             0.86|\n",
      "|  Century Palms/Cove|          8552.23|             1.27|\n",
      "|          Chatsworth|         30580.89|             0.69|\n",
      "+--------------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "139"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, first, regexp_replace, concat, lit, lower\n",
    "from sedona.spark import *\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 3\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona Context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_data_10s_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_20s_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "income_data_df = spark.read.csv(income_data_path,header=True,inferSchema=True)\n",
    "crime_data_10s = spark.read.csv(crime_data_10s_path,header=True,inferSchema=True)\n",
    "crime_data_20s = spark.read.csv(crime_data_20s_path,header=True,inferSchema=True)\n",
    "crime_data = crime_data_10s.union(crime_data_20s)\n",
    "\n",
    "# Load GeoJSON Data (2010 Census Blocks) from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "census_blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten GeoJSON properties\n",
    "flattened_census_blocks_df = census_blocks_df.select(\n",
    "    [col(f\"properties.{col_name}\").alias(col_name) for col_name in census_blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]\n",
    ").drop(\"properties\").drop(\"type\")\n",
    "\n",
    "census_geometry = flattened_census_blocks_df.filter(col(\"CITY\")== \"Los Angeles\").select(\"COMM\", \"HOUSING10\", \"POP_2010\", \"ZCTA10\", \"geometry\")\\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\")\\\n",
    "    .agg(ST_Union_Aggr(\"geometry\").alias(\"geometry\"), \n",
    "    sum(\"POP_2010\").alias(\"Total_POP_2010\"),\n",
    "    sum(\"HOUSING10\").alias(\"Total_HOUSING10\")\n",
    "        )\n",
    "\n",
    "#census_selected_columns_df.orderBy(\"COMM\").show()\n",
    "#census_selected_columns_df.count()\n",
    "\n",
    "# Remove the first character of the \"Estimated Median Income\" $\n",
    "income_data_df = income_data_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    col(\"Estimated Median Income\").substr(2, 100) # Keep characters starting from position 2\n",
    ")\n",
    "income_data_df = income_data_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \",\", \"\")\n",
    ")\n",
    "\n",
    "# Filter and clean Income Data (ensure ZCTA10 and COMM_income are not null)\n",
    "income_data_df = income_data_df.filter(\n",
    "    (col(\"Zip Code\").isNotNull()) & (col(\"Community\").isNotNull()) & (col(\"Estimated Median Income\").isNotNull())\n",
    ")\n",
    "\n",
    "crime_points = crime_data.filter(col(\"LAT\").isNotNull() & col(\"LON\").isNotNull())\\\n",
    "    .withColumn(\"geometry\",ST_Point(col(\"LON\"), col(\"LAT\"))) \\\n",
    "    .select(\"geometry\")\n",
    "\n",
    "# Perform spatial join: Find crimes inside polygons\n",
    "community_crimes = crime_points.join(\n",
    "    census_geometry.hint(\"BROADCAST\"),\n",
    "    ST_Within(crime_points[\"geometry\"], census_geometry[\"geometry\"]),\n",
    "    \"inner\"\n",
    ").groupBy(\"COMM\").agg(\n",
    "    count(\"*\").alias(\"Total Crimes\")\n",
    ")\n",
    "\n",
    "result_df = (\n",
    "    census_geometry.join(\n",
    "        income_data_df.hint(\"BROADCAST\"),\n",
    "        census_geometry[\"ZCTA10\"] == income_data_df[\"Zip Code\"],\n",
    "        \"inner\"\n",
    "    ).select(\"COMM\", \"ZCTA10\", \"Total_POP_2010\", \"Total_HOUSING10\",\"Estimated Median Income\")\n",
    "    .filter(\n",
    "        (col(\"Total_POP_2010\") != 0) & (col(\"Total_HOUSING10\") != 0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add \"Income per Person\" column with at most 2 decimals\n",
    "result_df = result_df.withColumn(\n",
    "    \"Income_Per_Person\",\n",
    "    round((col(\"Estimated Median Income\") * col(\"Total_HOUSING10\")) / col(\"Total_POP_2010\"), 2) \n",
    ")\n",
    "#result_df.orderBy(\"COMM\").show()\n",
    "\n",
    "total_population = result_df.groupBy(\"COMM\").agg(sum(\"Total_POP_2010\").alias(\"Community_Population\"))\n",
    "\n",
    "#total_population.orderBy(\"COMM\").show()\n",
    "\n",
    "community_income = result_df.join(\n",
    "    total_population.hint(\"BROADCAST\"),\n",
    "    on=\"COMM\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "community_income = community_income.withColumn(\"Community_Income\",\n",
    "                                 round((col(\"Total_POP_2010\") * col(\"Income_Per_Person\") \n",
    "            /col(\"Community_Population\")),2))\n",
    "\n",
    "community_income = (\n",
    "    community_income\n",
    "    .groupBy(\"COMM\")\n",
    "    .agg(\n",
    "        sum(\"Total_POP_2010\").alias(\"Community_Population\"),\n",
    "        round(sum(\"Community_Income\"), 2).alias(\"Community_Income\")\n",
    "    )\n",
    ")\n",
    "\n",
    "#community_income.orderBy(\"COMM\").show()\n",
    "\n",
    "result_df = (\n",
    "    community_income.join(\n",
    "        community_crimes.hint(\"BROADCAST\"),\n",
    "        on=\"COMM\",\n",
    "        how=\"inner\"  \n",
    "    )\n",
    "    .select(\n",
    "        col(\"COMM\").alias(\"Community\"),\n",
    "        col(\"Community_Population\"),\n",
    "        col(\"Community_Income\").alias(\"Income_Per_Person\"),\n",
    "        col(\"Total Crimes\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "#result_df.orderBy(\"COMM\").show()\n",
    "\n",
    "# Now calculate \"Crimes_Per_Person\" for each community\n",
    "result_df = result_df.withColumn(\n",
    "    \"Crimes_Per_Person\",\n",
    "    round(col(\"Total Crimes\") / col(\"Community_Population\"), 2)\n",
    ")\n",
    "\n",
    "final_df = result_df.select(\n",
    "    col(\"Community\"),\n",
    "    col(\"Income_Per_Person\"),\n",
    "    col(\"Crimes_Per_Person\")\n",
    ")\n",
    "\n",
    "final_df.orderBy(\"Community\").show()\n",
    "final_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80392cac-3831-4e70-92a5-8fc1281101b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ερώτημα 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a94947f0-3146-41a6-954a-956bd76dfbf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+------------+-----------------+\n",
      "|           Community|Community_Population|Income_Per_Person|Total Crimes|Crimes_Per_Person|\n",
      "+--------------------+--------------------+-----------------+------------+-----------------+\n",
      "|     Adams-Normandie|                7842|          8791.46|        5779|             0.74|\n",
      "|              Alsace|               11728|          11239.5|        6466|             0.55|\n",
      "|Angeles National ...|                  20|         28117.65|         227|            11.35|\n",
      "|    Angelino Heights|                2376|         18411.55|        1497|             0.63|\n",
      "|              Arleta|               32876|         12110.78|       14542|             0.44|\n",
      "|     Atwater Village|               14101|          28477.2|        9696|             0.69|\n",
      "|       Baldwin Hills|               28637|          17302.7|       34194|             1.19|\n",
      "|             Bel Air|                8261|         63041.34|        3532|             0.43|\n",
      "|       Beverly Crest|               12191|         60947.49|        4527|             0.37|\n",
      "|         Beverlywood|               12415|         29267.82|        6471|             0.52|\n",
      "|       Boyle Heights|               82536|          8434.46|       59656|             0.72|\n",
      "|           Brentwood|               29301|         60840.62|       14756|              0.5|\n",
      "|           Brookside|                 621|         18138.62|         550|             0.89|\n",
      "|    Cadillac-Corning|                6665|         19572.78|        4445|             0.67|\n",
      "|         Canoga Park|               58943|         19656.96|       52816|              0.9|\n",
      "|             Carthay|               13165|         49841.16|       12273|             0.93|\n",
      "|             Central|               35422|          6972.52|       24981|             0.71|\n",
      "|        Century City|               11890|         46103.51|       10176|             0.86|\n",
      "|  Century Palms/Cove|               30692|          8552.23|       38877|             1.27|\n",
      "|          Chatsworth|               34647|         30580.89|       23963|             0.69|\n",
      "+--------------------+--------------------+-----------------+------------+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "result_df.orderBy(\"Community\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "617017e1-491f-451e-bf9e-4ac0a7420c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|695|\n",
      "|               Other| 86|\n",
      "|Hispanic/Latin/Me...| 77|\n",
      "|             Unknown| 49|\n",
      "|               Black| 43|\n",
      "|         Other Asian| 22|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|3342|\n",
      "|               Black|1127|\n",
      "|               White| 428|\n",
      "|               Other| 252|\n",
      "|         Other Asian| 138|\n",
      "|             Unknown|  30|\n",
      "|American Indian/A...|  23|\n",
      "|              Korean|   4|\n",
      "|            Filipino|   3|\n",
      "|             Chinese|   3|\n",
      "|           Guamanian|   1|\n",
      "|        Asian Indian|   1|\n",
      "+--------------------+----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, col, when\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query4_4Cores_8GB\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load GeoJSON Data (2010 Census Blocks) from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "census_blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten GeoJSON properties\n",
    "flattened_census_blocks_df = census_blocks_df.select(\n",
    "    [col(f\"properties.{col_name}\").alias(col_name) for col_name in census_blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]\n",
    ").drop(\"properties\").drop(\"type\")\n",
    "\n",
    "\n",
    "census_geometry = flattened_census_blocks_df.filter(col(\"CITY\")== \"Los Angeles\").select(\"COMM\", \"ZCTA10\", \"geometry\")\\\n",
    "    .groupBy(\"COMM\")\\\n",
    "    .agg(ST_Union_Aggr(\"geometry\").alias(\"geometry\"))\n",
    "         \n",
    "#census_geometry.orderBy(\"COMM\").show()\n",
    "\n",
    "# Get the 3 highest incomes per person\n",
    "highest_income = final_df.orderBy(desc(\"Community_Income\")).limit(3)\n",
    "#highest_income.show()\n",
    "\n",
    "# Get the 3 lowest incomes per person\n",
    "lowest_income = final_df.orderBy(\"Community_Income\").limit(3)\n",
    "#lowest_income.show()\n",
    "\n",
    "# Select the geometry for the 3 communities with the highest income\n",
    "highest_income_geometry = (\n",
    "    highest_income\n",
    "    .join(census_geometry, final_df[\"Community\"] == census_geometry[\"COMM\"])\n",
    "    .select(final_df[\"Community\"], \"Income_Per_Person\", census_geometry[\"geometry\"])\n",
    ")\n",
    "\n",
    "#highest_income_geometry.show()\n",
    "\n",
    "# Select the geometry for the 3 communities with the lowest income\n",
    "lowest_income_geometry = (\n",
    "    lowest_income\n",
    "    .join(census_geometry, final_df[\"Community\"] == census_geometry[\"COMM\"])\n",
    "    .select(final_df[\"Community\"], \"Income_Per_Person\", census_geometry[\"geometry\"])\n",
    ")\n",
    "\n",
    "#lowest_income_geometry.show()\n",
    "\n",
    "# Load and process the data for 2010s\n",
    "crime_data_10s_path =  \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_10s = spark.read.csv(crime_data_10s_path\n",
    "   ,header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "\n",
    "crime_with_year = (\n",
    "    crime_data_10s\n",
    "    .withColumn(\"year\", col(\"DATE OCC\").substr(7, 4).cast(\"int\"))\n",
    "    .filter((col(\"year\") == 2015) & (col(\"Vict Descent\").isNotNull()) & (col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()))\n",
    "    .select(\n",
    "        col(\"DR_NO\"),\n",
    "        col(\"Vict Descent\"),\n",
    "        col(\"LAT\"),\n",
    "        col(\"LON\"),\n",
    "        col(\"year\")\n",
    "    )\n",
    ")\n",
    "\n",
    "#crime_with_year.show()\n",
    "\n",
    "crime_2015_points = crime_with_year.withColumn(\"point\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "#crime_2015_points.show()\n",
    "\n",
    "\n",
    "\n",
    "  # Perform the spatial join using ST_Within\n",
    "crimes_in_high_income = crime_2015_points.join(\n",
    "    highest_income_geometry,\n",
    "    ST_Within(crime_2015_points[\"point\"] ,highest_income_geometry[\"geometry\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    crime_2015_points[\"DR_NO\"],\n",
    "    crime_2015_points[\"Vict Descent\"],\n",
    "    crime_2015_points[\"LAT\"],\n",
    "    crime_2015_points[\"LON\"],\n",
    "    highest_income_geometry[\"Community\"],\n",
    "    highest_income_geometry[\"Income_Per_Person\"]\n",
    ")\n",
    "\n",
    "crimes_in_low_income = crime_2015_points.join(\n",
    "    lowest_income_geometry,  \n",
    "    ST_Within(crime_2015_points[\"point\"], lowest_income_geometry[\"geometry\"]),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    crime_2015_points[\"DR_NO\"],\n",
    "    crime_2015_points[\"Vict Descent\"],\n",
    "    crime_2015_points[\"LAT\"],\n",
    "    crime_2015_points[\"LON\"],\n",
    "    lowest_income_geometry[\"Community\"],  \n",
    "    lowest_income_geometry[\"Income_Per_Person\"]  \n",
    ")\n",
    "\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "# Build the when condition for all mappings\n",
    "descent_column = when(col(\"Vict Descent\") == \"A\", \"Other Asian\")\n",
    "for code, description in descent_mapping.items():\n",
    "    descent_column = descent_column.when(col(\"Vict Descent\") == code, description)\n",
    "\n",
    "# Replace codes with descriptions in the DataFrame\n",
    "highest_vict_descent = crimes_in_high_income.groupBy(\"Vict Descent\").count().withColumn(\n",
    "    \"Victim Descent\", descent_column\n",
    ").select(\n",
    "    col(\"Victim Descent\"),\n",
    "    col(\"count\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "# Replace codes with descriptions in the DataFrame for lowest income communities\n",
    "lowest_vict_descent = crimes_in_low_income.groupBy(\"Vict Descent\").count().withColumn(\n",
    "    \"Victim Descent\", descent_column\n",
    ").select(\n",
    "    col(\"Victim Descent\"),\n",
    "    col(\"count\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "highest_vict_descent.show()\n",
    "lowest_vict_descent.show()                                                            \n",
    "                                                            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fdac5-fde7-4ea9-97c6-0cb4ad17fa1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ερώτημα 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d77fd907-c5c0-4098-9470-8a4d24aeeba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------+------------------+\n",
      "|Closest Area Name|Number of Closest Crimes|   Median Distance|\n",
      "+-----------------+------------------------+------------------+\n",
      "|      77th Street|                  206784| 2.292726866601563|\n",
      "|        Southwest|                  192226|2.3723678063581852|\n",
      "|          Pacific|                  170903| 4.342278455155561|\n",
      "|          Central|                  166698| 1.093428009365451|\n",
      "|      N Hollywood|                  164532|2.6969736366261494|\n",
      "|        Southeast|                  161051|1.8476966788287785|\n",
      "|        Hollywood|                  150663| 1.439317682870132|\n",
      "|           Newton|                  148757|1.7463468819264603|\n",
      "|          Olympic|                  144962|1.9791002660074106|\n",
      "|          Mission|                  143600| 4.228196896970822|\n",
      "|        Northeast|                  142732| 3.972583911782645|\n",
      "|         Van Nuys|                  142194|2.5715909394184853|\n",
      "|          Topanga|                  138642|3.5032555344892358|\n",
      "|       Devonshire|                  137881|3.3232967291195066|\n",
      "|         Wilshire|                  136199|2.4558662959589768|\n",
      "|          Rampart|                  135948|1.3388016824808497|\n",
      "|          West LA|                  134259| 3.111039069941839|\n",
      "|           Harbor|                  132911| 3.716521204833013|\n",
      "|      West Valley|                  131502|3.4930493136928353|\n",
      "|       Hollenbeck|                  114517|2.4314867007517815|\n",
      "|         Foothill|                  112919|3.8182090552354824|\n",
      "+-----------------+------------------------+------------------+\n",
      "\n",
      "Execution time: 7.450225591659546 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, expr, first, desc\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "import time\n",
    "\n",
    "# Initialize Spark session and Sedona context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeDataAnalysis\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load police stations data\n",
    "police_stations_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "police_stations_df = spark.read.csv(\n",
    "    police_stations_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load and process the crime data for 2010s\n",
    "crime_data_10s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Load and process the crime data for 2020s\n",
    "crime_data_20s = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter for the required description and select necessary columns\n",
    "useful_10s = crime_data_10s.select(\n",
    "    col(\"DR_NO\").alias(\"Crime ID\"),\n",
    "    col(\"AREA \").alias(\"Area\"),\n",
    "    col(\"AREA NAME\").alias(\"Area Name\"),\n",
    "    col(\"LAT\").alias(\"Latitude\"),\n",
    "    col(\"LON\").alias(\"Longtitude\")\n",
    ")\n",
    "\n",
    "useful_20s = crime_data_20s.select(\n",
    "    col(\"DR_NO\").alias(\"Crime ID\"),\n",
    "    col(\"AREA\").alias(\"Area\"),\n",
    "    col(\"AREA NAME\").alias(\"Area Name\"),\n",
    "    col(\"LAT\").alias(\"Latitude\"),\n",
    "    col(\"LON\").alias(\"Longtitude\")\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_useful = useful_10s.union(useful_20s)\n",
    "\n",
    "# Filter out Null Island records\n",
    "filtered_crimes = combined_useful.filter(\n",
    "    (col(\"Latitude\") != 0) & (col(\"Longtitude\") != 0)\n",
    ")\n",
    "\n",
    "# Create geospatial points for crimes\n",
    "crime_points = filtered_crimes.withColumn(\n",
    "    \"crime_geom\", ST_Point(col(\"Longtitude\"), col(\"Latitude\"))\n",
    ")\n",
    "\n",
    "# Create geospatial points for police stations\n",
    "station_points = police_stations_df.withColumn(\n",
    "    \"station_geom\", ST_Point(col(\"X\"), col(\"Y\"))\n",
    ")\n",
    "\n",
    "# Perform a Cartesian join to calculate distances between each crime and police station\n",
    "\n",
    "# Earth's radius in kilometers\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "# Perform a Cartesian join to calculate distances in kilometers\n",
    "crime_station_distances = crime_points.crossJoin(station_points).withColumn(\n",
    "    \"distance\", \n",
    "    ST_Distance(col(\"crime_geom\"), col(\"station_geom\")) * (3.141592653589793 / 180) * EARTH_RADIUS_KM\n",
    ")\n",
    "#crime_station_distances.show()\n",
    "\n",
    "# Find the minimum distance for each crime and keep Area and Area Name\n",
    "min_distances = crime_station_distances.groupBy(\"Crime ID\").agg(\n",
    "    expr(\"min(distance)\").alias(\"min_distance\"),\n",
    "    first(\"Area\").alias(\"Closest Area\"),\n",
    "    first(\"Area Name\").alias(\"Closest Area Name\")\n",
    ")\n",
    "\n",
    "min_distances = min_distances.orderBy(\"Closest Area\")\n",
    "# Show the result\n",
    "#min_distances.show()\n",
    "#min_distances.count()\n",
    "\n",
    "# Step 1: Count of tuples for each closest area name\n",
    "area_counts = min_distances.groupBy(\"Closest Area Name\").agg(\n",
    "    count(\"Crime ID\").alias(\"Number of Closest Crimes\")\n",
    ")\n",
    "#area_counts.show(50)\n",
    "\n",
    "# Step 2: Median distance for each closest area name\n",
    "area_median_distances = min_distances.groupBy(\"Closest Area Name\").agg(\n",
    "    expr(\"percentile_approx(min_distance, 0.5)\").alias(\"Median Distance\")\n",
    ")\n",
    "\n",
    "# Step 3: Join counts and medians, and order by Number of Closest Crimes in descending order\n",
    "final_table = area_counts.join(\n",
    "    area_median_distances,\n",
    "    on=\"Closest Area Name\"\n",
    ").orderBy(desc(\"Number of Closest Crimes\"))\n",
    "\n",
    "final_table.show(21)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906f673-39ba-433a-97cb-f120335f3e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
